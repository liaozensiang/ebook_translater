# --- LLM API Configuration ---
# If using local GPU or a separate vLLM server:
LLM_API_URL=http://<SERVER_or_LOCAL_IP>:8000/v1
# If using an External API (e.g. OpenAI, DeepSeek):
# LLM_API_URL=https://api.openai.com/v1
# LLM_API_URL=https://api.deepseek.com/v1

LLM_API_KEY=sk-test-123

# Default Model Name (Must match what is served/requested)
LLM_MODEL=Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4

# --- Web Interface ---
# Port for the Review Web UI
APP_PORT=5000

# --- vLLM Local Server Settings (Used by run_vllm.sh) ---
# Token for accessing gated models
HUGGING_FACE_HUB_TOKEN=

# --- Path Configuration (Used by Workflow Scripts) ---
# For Step 1: Alignment
ALIGN_SOURCE_EPUB=source/original.epub
ALIGN_REF_EPUB=reference/reference.epub
ALIGN_OUTPUT_GLOSSARY=glossary.json

# For Step 2-5: Translation Flow
INPUT_EPUB=source/new.epub
OUTPUT_EPUB=output/translated.epub
# Language Definitions (Passed to LLM)
SRC_LANG="Japanese"
TGT_LANG="Traditional Chinese"

# Work Directory for Review Session
WORK_DIR=/app/work_session

# GPU Memory Utilization (0.95 is standard for dedicated GPUs)
GPU_MEMORY_UTILIZATION=0.8

# Context Window Size
MAX_MODEL_LEN=8192
